{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR on classifying cookie banner text\n",
    "- The goal is to see if we could get a Naive-Bayes implementation to work with data gathered from various sources on good and bad cookie banners, manually collected and labeled based on literature from CS 194\n",
    "- We were able to get it to work, however the results aren't satisfactory and we've hit a wall with how we're better able to pre-process the data to effectively isolate important keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('combined_ms.csv')\n",
    "data = df.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the text\n",
    "- Removing punctuation\n",
    "- Should we take out greetings and the word \"cookies\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['We', 'use', 'cookies', 'and', 'other', 'tracking', 'technologies', 'to', 'improve', 'your', 'browsing', 'experience', 'on', 'our', 'site', 'show', 'personalized', 'content', 'and', 'targeted', 'ads', 'analyze', 'site', 'traffic', 'and', 'understand', 'where', 'our', 'audience', 'is', 'coming', 'from', 'To', 'find', 'out', 'more', 'or', 'to', 'optout', 'please', 'read', 'our', 'Cookie', 'Policy', 'In', 'addition', 'please', 'read', 'our', 'Privacy', 'Policy', 'which', 'has', 'also', 'been', 'updated', 'and', 'became', 'effective', 'May', '23rd', '2018', 'By', 'choosing', 'I', 'Accept', 'you', 'consent', 'to', 'our', 'use', 'of', 'cookies', 'and', 'other', 'tracking', 'technologies'], 'GOOD')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "word_string = ''\n",
    "documents = []\n",
    "\n",
    "for point in data:\n",
    "    curr_tuple = (point[0].translate(str.maketrans('', '', string.punctuation)).split(), point[1])\n",
    "    documents.append(curr_tuple)\n",
    "    word_string = word_string + point[0].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "random.shuffle(documents)\n",
    "print(documents[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the feature extractor\n",
    "- Takes five of the words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Define the feature extractor\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in word_string.split())\n",
    "\n",
    "word_features = list(all_words)[:5]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the classifier\n",
    "- Using NLTK to split the data into testing sets and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[80:], featuresets[:80]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features\n",
    "- Noticing they're repeated, doing some reading as to how we can better isolate \"relevant\" words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(and) = False             BAD : GOOD   =     18.4 : 1.0\n",
      "           contains(our) = False             BAD : GOOD   =      3.9 : 1.0\n",
      "           contains(and) = True             GOOD : BAD    =      2.0 : 1.0\n",
      "            contains(to) = False            GOOD : BAD    =      1.3 : 1.0\n",
      "           contains(our) = True             GOOD : BAD    =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show the most important features as interpreted by Naive Bayes\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "save_classifier = open('calliope.pickle', 'wb')\n",
    "pickle.dump(classifier, save_classifier) \n",
    "save_classifier.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
